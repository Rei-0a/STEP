## 1. 概要

- **課題名**:
- **作成日**: 2025年6月13日

## 2. 各課題の内容

Wikipediaから知識を発見する

日本語のWikipediaの全ページのタイトルとリンクの関係のデータセットを用いる
(ページ数：2,215,900)
(リンク数：119,006,494)

[サンプルコード](https://github.com/xharaken/step2/blob/master/wikipedia.py)内で、データセットの読み込みを実装済みであるため、グラフ解析のコードだけ書けばよい。

### 課題1

`find_shortest_path()`関数を書いて、あるページから別のページへの最短経路を出力する

- ヒント💡
  - BFSに工夫を入れて最短経路を出せるようにする
  - `collections.deque`を使うとスタックやキューが作れる
  - 30行程度で書ける

### 課題2

`find_most_popular_pages()`関数を書いて、ページランクを計算して重要度の高いページトップ10を求める
    - 「言葉で説明したアルゴリズムを自分で具体化してコードに落とす」ことが宿題の意図
    - 50行程度で書ける

- ヒント💡
  - ページランクの分配と更新を何回繰り返しても「全体のノードのページランクの合計値」が一定に保たれることを確認
    - 一定にならない場合、なにかが間違っている！
  - Largeのデータセットで動かすためには `O(N+E)`のアルゴリズムが必要
    - ページ数 `N=2215900`
    - リンク数 `E=119006494`
  - ページランクの更新が「完全に」収束するのは時間がかかりすぎるので、更新が十分少なくなったら止める
  - 収束条件の作り方の例:
    - `\sigma (new_pagerank[i]-old_pagelank[i])^2 < 0.01 `

## 3. Goals and Non-Goals

- 一般ユーザー
- スマートフォンからアクセスするユーザー

## 4. 提案する設計


`pages.txt`のデータの入り方
```(python)
self.titles = {
    5 : "アンパサンド"
    10 : "言語"
    11 : "日本語"
    12 : "地理学"
    13 : "EU_(曖昧さ回避)"
    ...
}
```

`links.txt`のデータの入り方
```(python)
self.links = {
    5 69
    5 638
    5 1022
    5 1136
    5 1487
    ...
}
```


## 5. 代替案の検討

## 6. Open Questions
