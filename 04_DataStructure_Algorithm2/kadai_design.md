## 1. 概要

- **課題名**:
- **作成日**: 2025年6月13日

## 2. 各課題の内容

Wikipediaから知識を発見する

日本語のWikipediaの全ページのタイトルとリンクの関係のデータセットを用いる
(ページ数：2,215,900)
(リンク数：119,006,494)

[サンプルコード](https://github.com/xharaken/step2/blob/master/wikipedia.py)内で、データセットの読み込みを実装済みであるため、グラフ解析のコードだけ書けばよい。

### 課題1

`find_shortest_path()`関数を書いて、あるページから別のページへの最短経路を出力する

- ヒント💡
  - BFSに工夫を入れて最短経路を出せるようにする
  - `collections.deque`を使うとスタックやキューが作れる
  - 30行程度で書ける

### 課題2

`find_most_popular_pages()`関数を書いて、ページランクを計算して重要度の高いページトップ10を求める
    - 「言葉で説明したアルゴリズムを自分で具体化してコードに落とす」ことが宿題の意図
    - 50行程度で書ける

- ヒント💡
  - ページランクの分配と更新を何回繰り返しても「全体のノードのページランクの合計値」が一定に保たれることを確認
    - 一定にならない場合、なにかが間違っている！
  - Largeのデータセットで動かすためには `O(N+E)`のアルゴリズムが必要
    - ページ数 `N=2215900`
    - リンク数 `E=119006494`
  - ページランクの更新が「完全に」収束するのは時間がかかりすぎるので、更新が十分少なくなったら止める
  - 収束条件の作り方の例:
    - `\sigma (new_pagerank[i]-old_pagelank[i])^2 < 0.01 `

## 3. Goals and Non-Goals

### Goals

- 課題の条件を満たすこと
- 存在しないページを入力されたときに、存在しないと出力する

## 4. 提案する設計

`pages.txt`のデータを、コード内では、以下のように読み取っている。

```(python)
self.titles = {
    5 : "アンパサンド"
    10 : "言語"
    11 : "日本語"
    12 : "地理学"
    13 : "EU_(曖昧さ回避)"
    ...
}
```

`links.txt`のデータを、コード内では、以下のように読み取っている。

```(python)
self.links = {
    5 69
    5 638
    5 1022
    5 1136
    5 1487
    ...
}
```

### 課題1

`start`のページから `goal`のページまで、最短経路を探索する。
そのために、OPENリストをキュー型にするBFS(幅優先探索)を用いた。その探索を行う関数として、`find_shortest_path`関数を作成した。

##### 'find_shortest_path'関数(引数：start, goal)

1. ページIDの取得

- 与えられた `start`と `goal`のページタイトルから、それぞれ対応するページID（start_id, goal_id）を取得する

2. 探索用データ構造の初期化

- `visited` : 今まで訪れたページを保存する辞書
- `previous` : 探索待ちのページを保存する辞書

3. キューの初期手順

- `start_id`をキューに追加
- `visited`に `start_id`を追加
- `previous`に `start_id: None`を追加(スタートのページなので、親はいない)

4. キューが空になるまで以下を繰り返す
   a. キューの先頭からページID(`node`)を取り出す
   b. `node`が `goal`と一致するなら、5. へ(探索完了)
   c. `node`のリンクにあるページ(子ページ)を確認
   - `viseted`にその子ページがなければ、
     - キューにその子ページをいれる
     - `visited`へ入れる
     - `previous[child]=node`として、親ノードを保存
5. `previous`の `key`に `goal_id`があれば、`previpus`内を辿って経路を出力する。なければ、存在しないと出力する。

### 課題2

`find_most_popular_pages`関数を用いて、`page_rank`がもっとも高い、popularなページを出力した。

#### `find_most_popular_pages`関数

1. 辞書構造の初期化
   各ページのID(`title_key`)に対して、初期のページランクを `1`に設定。また、ページランクを更新するための辞書 `new_page_ranks`も作成。

- `old_page_ranks[title_key] = 1`
- `new_page_ranks[title_key] = 0`

2. ループで用いる変数の初期化

- `new_page_ranks`辞書を全て0にする
- 全体に分配するためのページランクを保存する変数 `all_rank_share`を0にする

3. 各ページに対して以下を繰り返す

- そのページからのリンクがあるとき
  - ページランクの85%を子ページに分配。`new_page_ranks`にいれておく。
  - 残り15%のページランクを `all_rank_share`に保存しておく
- そのページからのリンクがないとき
  - `all_rank_share`にそのページランクを100%保存しておく

4. 各ページに対して以下を繰り返す

- `new_page_ranks`の値に、`all_rank_share`を分配(全ページ数で割った値を足す)
- `delta`に、`new_page_ranks`と `old_page_ranks`の差分の二乗を足す

5. `delta < 0.01`となれば、ループを終了。そうでなければ2. に戻る
6. `new_page_ranks`の中から、値が大きいものから10個を取り出し、出力する

## 5. 実行結果

`small`のとき
A -> B -> C -> F
Most popular page by PageRank:
C 1.3730703124999994
D 1.3730703124999994
B 1.196673177083333
E 0.811782552083333
F 0.811782552083333
A 0.4336210937499999

`medium`のとき
渋谷 -> ギャルサー_(テレビドラマ) -> 小野妹子
Most popular page by PageRank:
英語 1507.297700556478
ISBN 959.7071288904509
2006年 526.1013565988807
2005年 502.26093241761237
2007年 491.481850589156
東京都 480.2739485131828
昭和 459.3758140702666
2004年 445.3697475728473
2003年 404.73835956539045
2000年 401.88955444725576

`large`のとき
渋谷 -> ギャルサー_(テレビドラマ) -> 小野妹子
Most popular page by PageRank:
英語 4576.828626117526
日本 4569.220860331079
VIAF_(識別子) 3806.8189930091244
バーチャル国際典拠ファイル 3320.3658112865664
アメリカ合衆国 2714.5298344096527
ISBN 2711.526648514297
ISNI_(識別子) 2060.6625996408834
国際標準名称識別子 1865.4484409757633
地理座標系 1815.8479149648497
SUDOC_(識別子) 1518.7882366990211

## 6. Open Questions

`medium`も `large`もとてつもなく時間がかかるが、これはしょうがないことなのか？
